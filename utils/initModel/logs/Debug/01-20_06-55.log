Some weights of IModelForCausalLM were not initialized from the model checkpoint at /sds_wangby/models/Phoenix-II/baseModels/doNotUpload/Qwen2.5-0.5B and are newly initialized: ['model.layers.0.input_layernorms.0.weight', 'model.layers.0.input_layernorms.1.weight', 'model.layers.0.post_attention_layernorms.0.weight', 'model.layers.0.self_attns.0.k_proj.bias', 'model.layers.0.self_attns.0.k_proj.weight', 'model.layers.0.self_attns.0.o_proj.weight', 'model.layers.0.self_attns.0.q_proj.bias', 'model.layers.0.self_attns.0.q_proj.weight', 'model.layers.0.self_attns.0.v_proj.bias', 'model.layers.0.self_attns.0.v_proj.weight', 'model.layers.1.input_layernorms.0.weight', 'model.layers.1.input_layernorms.1.weight', 'model.layers.1.post_attention_layernorms.0.weight', 'model.layers.1.self_attns.0.k_proj.bias', 'model.layers.1.self_attns.0.k_proj.weight', 'model.layers.1.self_attns.0.o_proj.weight', 'model.layers.1.self_attns.0.q_proj.bias', 'model.layers.1.self_attns.0.q_proj.weight', 'model.layers.1.self_attns.0.v_proj.bias', 'model.layers.1.self_attns.0.v_proj.weight', 'model.layers.10.input_layernorms.0.weight', 'model.layers.10.input_layernorms.1.weight', 'model.layers.10.post_attention_layernorms.0.weight', 'model.layers.10.self_attns.0.k_proj.bias', 'model.layers.10.self_attns.0.k_proj.weight', 'model.layers.10.self_attns.0.o_proj.weight', 'model.layers.10.self_attns.0.q_proj.bias', 'model.layers.10.self_attns.0.q_proj.weight', 'model.layers.10.self_attns.0.v_proj.bias', 'model.layers.10.self_attns.0.v_proj.weight', 'model.layers.11.input_layernorms.0.weight', 'model.layers.11.input_layernorms.1.weight', 'model.layers.11.post_attention_layernorms.0.weight', 'model.layers.11.self_attns.0.k_proj.bias', 'model.layers.11.self_attns.0.k_proj.weight', 'model.layers.11.self_attns.0.o_proj.weight', 'model.layers.11.self_attns.0.q_proj.bias', 'model.layers.11.self_attns.0.q_proj.weight', 'model.layers.11.self_attns.0.v_proj.bias', 'model.layers.11.self_attns.0.v_proj.weight', 'model.layers.12.input_layernorms.0.weight', 'model.layers.12.input_layernorms.1.weight', 'model.layers.12.post_attention_layernorms.0.weight', 'model.layers.12.self_attns.0.k_proj.bias', 'model.layers.12.self_attns.0.k_proj.weight', 'model.layers.12.self_attns.0.o_proj.weight', 'model.layers.12.self_attns.0.q_proj.bias', 'model.layers.12.self_attns.0.q_proj.weight', 'model.layers.12.self_attns.0.v_proj.bias', 'model.layers.12.self_attns.0.v_proj.weight', 'model.layers.13.input_layernorms.0.weight', 'model.layers.13.input_layernorms.1.weight', 'model.layers.13.post_attention_layernorms.0.weight', 'model.layers.13.self_attns.0.k_proj.bias', 'model.layers.13.self_attns.0.k_proj.weight', 'model.layers.13.self_attns.0.o_proj.weight', 'model.layers.13.self_attns.0.q_proj.bias', 'model.layers.13.self_attns.0.q_proj.weight', 'model.layers.13.self_attns.0.v_proj.bias', 'model.layers.13.self_attns.0.v_proj.weight', 'model.layers.14.input_layernorms.0.weight', 'model.layers.14.input_layernorms.1.weight', 'model.layers.14.post_attention_layernorms.0.weight', 'model.layers.14.self_attns.0.k_proj.bias', 'model.layers.14.self_attns.0.k_proj.weight', 'model.layers.14.self_attns.0.o_proj.weight', 'model.layers.14.self_attns.0.q_proj.bias', 'model.layers.14.self_attns.0.q_proj.weight', 'model.layers.14.self_attns.0.v_proj.bias', 'model.layers.14.self_attns.0.v_proj.weight', 'model.layers.15.input_layernorms.0.weight', 'model.layers.15.input_layernorms.1.weight', 'model.layers.15.post_attention_layernorms.0.weight', 'model.layers.15.self_attns.0.k_proj.bias', 'model.layers.15.self_attns.0.k_proj.weight', 'model.layers.15.self_attns.0.o_proj.weight', 'model.layers.15.self_attns.0.q_proj.bias', 'model.layers.15.self_attns.0.q_proj.weight', 'model.layers.15.self_attns.0.v_proj.bias', 'model.layers.15.self_attns.0.v_proj.weight', 'model.layers.16.input_layernorms.0.weight', 'model.layers.16.input_layernorms.1.weight', 'model.layers.16.post_attention_layernorms.0.weight', 'model.layers.16.self_attns.0.k_proj.bias', 'model.layers.16.self_attns.0.k_proj.weight', 'model.layers.16.self_attns.0.o_proj.weight', 'model.layers.16.self_attns.0.q_proj.bias', 'model.layers.16.self_attns.0.q_proj.weight', 'model.layers.16.self_attns.0.v_proj.bias', 'model.layers.16.self_attns.0.v_proj.weight', 'model.layers.17.input_layernorms.0.weight', 'model.layers.17.input_layernorms.1.weight', 'model.layers.17.post_attention_layernorms.0.weight', 'model.layers.17.self_attns.0.k_proj.bias', 'model.layers.17.self_attns.0.k_proj.weight', 'model.layers.17.self_attns.0.o_proj.weight', 'model.layers.17.self_attns.0.q_proj.bias', 'model.layers.17.self_attns.0.q_proj.weight', 'model.layers.17.self_attns.0.v_proj.bias', 'model.layers.17.self_attns.0.v_proj.weight', 'model.layers.18.input_layernorms.0.weight', 'model.layers.18.input_layernorms.1.weight', 'model.layers.18.post_attention_layernorms.0.weight', 'model.layers.18.self_attns.0.k_proj.bias', 'model.layers.18.self_attns.0.k_proj.weight', 'model.layers.18.self_attns.0.o_proj.weight', 'model.layers.18.self_attns.0.q_proj.bias', 'model.layers.18.self_attns.0.q_proj.weight', 'model.layers.18.self_attns.0.v_proj.bias', 'model.layers.18.self_attns.0.v_proj.weight', 'model.layers.19.input_layernorms.0.weight', 'model.layers.19.input_layernorms.1.weight', 'model.layers.19.post_attention_layernorms.0.weight', 'model.layers.19.self_attns.0.k_proj.bias', 'model.layers.19.self_attns.0.k_proj.weight', 'model.layers.19.self_attns.0.o_proj.weight', 'model.layers.19.self_attns.0.q_proj.bias', 'model.layers.19.self_attns.0.q_proj.weight', 'model.layers.19.self_attns.0.v_proj.bias', 'model.layers.19.self_attns.0.v_proj.weight', 'model.layers.2.input_layernorms.0.weight', 'model.layers.2.input_layernorms.1.weight', 'model.layers.2.post_attention_layernorms.0.weight', 'model.layers.2.self_attns.0.k_proj.bias', 'model.layers.2.self_attns.0.k_proj.weight', 'model.layers.2.self_attns.0.o_proj.weight', 'model.layers.2.self_attns.0.q_proj.bias', 'model.layers.2.self_attns.0.q_proj.weight', 'model.layers.2.self_attns.0.v_proj.bias', 'model.layers.2.self_attns.0.v_proj.weight', 'model.layers.20.input_layernorms.0.weight', 'model.layers.20.input_layernorms.1.weight', 'model.layers.20.post_attention_layernorms.0.weight', 'model.layers.20.self_attns.0.k_proj.bias', 'model.layers.20.self_attns.0.k_proj.weight', 'model.layers.20.self_attns.0.o_proj.weight', 'model.layers.20.self_attns.0.q_proj.bias', 'model.layers.20.self_attns.0.q_proj.weight', 'model.layers.20.self_attns.0.v_proj.bias', 'model.layers.20.self_attns.0.v_proj.weight', 'model.layers.21.input_layernorms.0.weight', 'model.layers.21.input_layernorms.1.weight', 'model.layers.21.post_attention_layernorms.0.weight', 'model.layers.21.self_attns.0.k_proj.bias', 'model.layers.21.self_attns.0.k_proj.weight', 'model.layers.21.self_attns.0.o_proj.weight', 'model.layers.21.self_attns.0.q_proj.bias', 'model.layers.21.self_attns.0.q_proj.weight', 'model.layers.21.self_attns.0.v_proj.bias', 'model.layers.21.self_attns.0.v_proj.weight', 'model.layers.22.input_layernorms.0.weight', 'model.layers.22.input_layernorms.1.weight', 'model.layers.22.post_attention_layernorms.0.weight', 'model.layers.22.self_attns.0.k_proj.bias', 'model.layers.22.self_attns.0.k_proj.weight', 'model.layers.22.self_attns.0.o_proj.weight', 'model.layers.22.self_attns.0.q_proj.bias', 'model.layers.22.self_attns.0.q_proj.weight', 'model.layers.22.self_attns.0.v_proj.bias', 'model.layers.22.self_attns.0.v_proj.weight', 'model.layers.23.input_layernorms.0.weight', 'model.layers.23.input_layernorms.1.weight', 'model.layers.23.post_attention_layernorms.0.weight', 'model.layers.23.self_attns.0.k_proj.bias', 'model.layers.23.self_attns.0.k_proj.weight', 'model.layers.23.self_attns.0.o_proj.weight', 'model.layers.23.self_attns.0.q_proj.bias', 'model.layers.23.self_attns.0.q_proj.weight', 'model.layers.23.self_attns.0.v_proj.bias', 'model.layers.23.self_attns.0.v_proj.weight', 'model.layers.3.input_layernorms.0.weight', 'model.layers.3.input_layernorms.1.weight', 'model.layers.3.post_attention_layernorms.0.weight', 'model.layers.3.self_attns.0.k_proj.bias', 'model.layers.3.self_attns.0.k_proj.weight', 'model.layers.3.self_attns.0.o_proj.weight', 'model.layers.3.self_attns.0.q_proj.bias', 'model.layers.3.self_attns.0.q_proj.weight', 'model.layers.3.self_attns.0.v_proj.bias', 'model.layers.3.self_attns.0.v_proj.weight', 'model.layers.4.input_layernorms.0.weight', 'model.layers.4.input_layernorms.1.weight', 'model.layers.4.post_attention_layernorms.0.weight', 'model.layers.4.self_attns.0.k_proj.bias', 'model.layers.4.self_attns.0.k_proj.weight', 'model.layers.4.self_attns.0.o_proj.weight', 'model.layers.4.self_attns.0.q_proj.bias', 'model.layers.4.self_attns.0.q_proj.weight', 'model.layers.4.self_attns.0.v_proj.bias', 'model.layers.4.self_attns.0.v_proj.weight', 'model.layers.5.input_layernorms.0.weight', 'model.layers.5.input_layernorms.1.weight', 'model.layers.5.post_attention_layernorms.0.weight', 'model.layers.5.self_attns.0.k_proj.bias', 'model.layers.5.self_attns.0.k_proj.weight', 'model.layers.5.self_attns.0.o_proj.weight', 'model.layers.5.self_attns.0.q_proj.bias', 'model.layers.5.self_attns.0.q_proj.weight', 'model.layers.5.self_attns.0.v_proj.bias', 'model.layers.5.self_attns.0.v_proj.weight', 'model.layers.6.input_layernorms.0.weight', 'model.layers.6.input_layernorms.1.weight', 'model.layers.6.post_attention_layernorms.0.weight', 'model.layers.6.self_attns.0.k_proj.bias', 'model.layers.6.self_attns.0.k_proj.weight', 'model.layers.6.self_attns.0.o_proj.weight', 'model.layers.6.self_attns.0.q_proj.bias', 'model.layers.6.self_attns.0.q_proj.weight', 'model.layers.6.self_attns.0.v_proj.bias', 'model.layers.6.self_attns.0.v_proj.weight', 'model.layers.7.input_layernorms.0.weight', 'model.layers.7.input_layernorms.1.weight', 'model.layers.7.post_attention_layernorms.0.weight', 'model.layers.7.self_attns.0.k_proj.bias', 'model.layers.7.self_attns.0.k_proj.weight', 'model.layers.7.self_attns.0.o_proj.weight', 'model.layers.7.self_attns.0.q_proj.bias', 'model.layers.7.self_attns.0.q_proj.weight', 'model.layers.7.self_attns.0.v_proj.bias', 'model.layers.7.self_attns.0.v_proj.weight', 'model.layers.8.input_layernorms.0.weight', 'model.layers.8.input_layernorms.1.weight', 'model.layers.8.post_attention_layernorms.0.weight', 'model.layers.8.self_attns.0.k_proj.bias', 'model.layers.8.self_attns.0.k_proj.weight', 'model.layers.8.self_attns.0.o_proj.weight', 'model.layers.8.self_attns.0.q_proj.bias', 'model.layers.8.self_attns.0.q_proj.weight', 'model.layers.8.self_attns.0.v_proj.bias', 'model.layers.8.self_attns.0.v_proj.weight', 'model.layers.9.input_layernorms.0.weight', 'model.layers.9.input_layernorms.1.weight', 'model.layers.9.post_attention_layernorms.0.weight', 'model.layers.9.self_attns.0.k_proj.bias', 'model.layers.9.self_attns.0.k_proj.weight', 'model.layers.9.self_attns.0.o_proj.weight', 'model.layers.9.self_attns.0.q_proj.bias', 'model.layers.9.self_attns.0.q_proj.weight', 'model.layers.9.self_attns.0.v_proj.bias', 'model.layers.9.self_attns.0.v_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Start Model Reintialization
[2025-01-20 06:55:43,146] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/sds_wangby/group_conda_envs/anaconda3/envs/fapy310/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(ctx, input, weight, bias=None):
/sds_wangby/group_conda_envs/anaconda3/envs/fapy310/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4
[93m [WARNING] [0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible
iModel is saved in:
/sds_wangby/models/Phoenix-II/baseModels/doNotUpload/Debug 
 copy file:
[]
